import os
import requests
from dotenv import load_dotenv

load_dotenv()  # Táº£i cÃ¡c biáº¿n mÃ´i trÆ°á»ng tá»« file .env

LLM_API_URL = os.getenv("LLM_API_URL", "http://host.docker.internal:11434/api/generate")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "nous-hermes2")

def stream_llm(question, context="KhÃ´ng cÃ³ ngá»¯ cáº£nh."):
    if not context.strip():
        prompt = question
    else:
         prompt = f"""
            Báº¡n lÃ  trá»£ lÃ½ AI thÃ´ng minh, chuyÃªn tráº£ lá»i cÃ¢u há»i dá»±a trÃªn dá»¯ liá»‡u ná»™i bá»™ Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t thÃ nh cÃ¡c Ä‘oáº¡n vÄƒn báº£n (chunk).

            DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘oáº¡n vÄƒn báº£n cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¢u há»i, Ä‘Æ°á»£c láº¥y tá»« tÃ¬m kiáº¿m Qdrant. 
            Má»™t sá»‘ Ä‘oáº¡n cÃ³ thá»ƒ trÃ¹ng láº·p á»Ÿ pháº§n Ä‘áº§u/cuá»‘i Ä‘á»ƒ giá»¯ ngá»¯ cáº£nh.

            ğŸ“Œ **NguyÃªn táº¯c tráº£ lá»i:**
            1. Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong cÃ¡c Ä‘oáº¡n vÄƒn báº£n bÃªn dÆ°á»›i.
            2. Bá» qua cÃ¡c thÃ´ng tin dÆ° thá»«a, khÃ´ng liÃªn quan hoáº·c trÃ¹ng láº·p.
            3. Náº¿u nhiá»u Ä‘oáº¡n liÃªn quan, hÃ£y tá»•ng há»£p chÃºng thÃ nh cÃ¢u tráº£ lá»i máº¡ch láº¡c, Ä‘áº§y Ä‘á»§ nhÆ°ng sÃºc tÃ­ch.
            4. Tuyá»‡t Ä‘á»‘i khÃ´ng thÃªm hoáº·c suy Ä‘oÃ¡n thÃ´ng tin khÃ´ng cÃ³ trong dá»¯ liá»‡u.
            5. Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i, tráº£ vá»: **"TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong dá»¯ liá»‡u ná»™i bá»™."**

            ---
            VÄƒn báº£n:
            \"\"\"{context}\"\"\"

            CÃ¢u há»i: {question}

            Tráº£ lá»i:
            """
    response = requests.post(
        LLM_API_URL,
        json={"model": LLM_MODEL_NAME, "prompt": prompt, "stream": True},
        stream=True,
    )

    for line in response.iter_lines():
        if line:
            try:
                data = line.decode("utf-8")
                if '"response":"' in data:
                    start = data.find('"response":"') + len('"response":"')
                    end = data.find('","', start)
                    chunk = data[start:end] if end != -1 else data[start:]
                    yield chunk
            except Exception:
                continue

def judge_question_relevance(question: str, context: str) -> bool:
    prompt = f"""
Báº¡n lÃ  há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ truy xuáº¥t ngá»¯ cáº£nh.

 DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘oáº¡n vÄƒn báº£n cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¢u há»i, Ä‘Æ°á»£c láº¥y tá»« tÃ¬m kiáº¿m Qdrant:
\"\"\"{context}\"\"\" 

VÃ  cÃ¢u há»i:
\"\"\"{question}\"\"\" 

CÃ¢u há»i nÃ y cÃ³ liÃªn quan vá»›i Ä‘oáº¡n vÄƒn báº£n trÃªn khÃ´ng?

Chá»‰ tráº£ lá»i duy nháº¥t má»™t tá»«:
- Náº¿u cÃ³ â†’ "yes"
- Náº¿u khÃ´ng liÃªn quan â†’ "no"
"""

    try:
        response = requests.post(
            LLM_API_URL,
            json={"model": LLM_MODEL_NAME, "prompt": prompt, "stream": False},
            timeout=300.0,
        )
        answer = response.json().get("response", "").strip().lower()
        return "yes" in answer
    except Exception as e:
        print("judge_question_relevance failed:", e)
        return False


